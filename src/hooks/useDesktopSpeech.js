// src/hooks/useDesktopSpeech.js
import { useState, useCallback, useEffect, useRef } from 'react';

export const useDesktopSpeech = (voiceEnabled = true) => {
  const [voiceState, setVoiceState] = useState({
    isListening: false,
    transcript: '',
    finalTranscript: '',
    isSupported: false,
    permissionState: 'prompt',
    status: 'idle',
    supportLevel: 'none',
    waitingForPermission: false
  });
  
  const [voiceError, setVoiceError] = useState(null);
  const recognitionRef = useRef(null);
  const finalTranscriptRef = useRef('');

  // æ£€æŸ¥è¯­éŸ³æ”¯æŒ
  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const isSupported = !!SpeechRecognition;
    
    setVoiceState(prev => ({
      ...prev,
      isSupported,
      supportLevel: isSupported ? 'good' : 'none'
    }));
    
    if (!isSupported) {
      setVoiceError('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚è¯·ä½¿ç”¨æœ€æ–°ç‰ˆChromeã€Edgeæˆ–Safariã€‚');
      return;
    }
    
    // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
    try {
      recognitionRef.current = new SpeechRecognition();
      const recognition = recognitionRef.current;
      
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.lang = 'zh-CN';
      recognition.maxAlternatives = 1;
      
      recognition.onstart = () => {
        console.log('ðŸ”Š æ¡Œé¢ç«¯è¯­éŸ³è¯†åˆ«å¼€å§‹');
        setVoiceState(prev => ({
          ...prev,
          isListening: true,
          status: 'listening',
          waitingForPermission: false
        }));
        setVoiceError(null);
      };
      
      recognition.onresult = (event) => {
        let interimTranscript = '';
        let finalTranscript = finalTranscriptRef.current;
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }
        
        finalTranscriptRef.current = finalTranscript;
        
        setVoiceState(prev => ({
          ...prev,
          transcript: interimTranscript,
          finalTranscript: finalTranscript
        }));
      };
      
      recognition.onerror = (event) => {
        console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error);
        
        let errorMessage = 'è¯­éŸ³è¯†åˆ«é”™è¯¯';
        switch (event.error) {
          case 'not-allowed':
          case 'permission-denied':
            errorMessage = 'éº¦å…‹é£Žæƒé™è¢«æ‹’ç»ã€‚è¯·å…è®¸æµè§ˆå™¨è®¿é—®éº¦å…‹é£Žã€‚';
            setVoiceState(prev => ({ ...prev, permissionState: 'denied' }));
            break;
          case 'no-speech':
            errorMessage = 'æœªæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥ã€‚è¯·æ£€æŸ¥éº¦å…‹é£Žè®¾ç½®ã€‚';
            break;
          case 'audio-capture':
            errorMessage = 'æœªæ‰¾åˆ°éº¦å…‹é£Žè®¾å¤‡ã€‚';
            break;
          default:
            errorMessage = `è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`;
        }
        
        setVoiceError(errorMessage);
        setVoiceState(prev => ({
          ...prev,
          isListening: false,
          status: 'error'
        }));
      };
      
      recognition.onend = () => {
        console.log('ðŸ”Š æ¡Œé¢ç«¯è¯­éŸ³è¯†åˆ«ç»“æŸ');
        setVoiceState(prev => ({
          ...prev,
          isListening: false,
          status: 'idle'
        }));
        
        // å¦‚æžœæœ‰æœ€ç»ˆè½¬å½•æ–‡æœ¬ï¼Œä¿ç•™å®ƒ
        if (finalTranscriptRef.current) {
          setTimeout(() => {
            finalTranscriptRef.current = '';
          }, 1000);
        }
      };
      
    } catch (error) {
      console.error('åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
      setVoiceError(`åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å¤±è´¥: ${error.message}`);
    }
  }, []);

  const startVoiceInput = useCallback(async () => {
    if (!recognitionRef.current) {
      setVoiceError('è¯­éŸ³è¯†åˆ«æœªåˆå§‹åŒ–');
      return;
    }
    
    if (voiceState.isListening) {
      console.log('è¯­éŸ³è¯†åˆ«å·²åœ¨è¿è¡Œä¸­');
      return;
    }
    
    // é‡ç½®çŠ¶æ€
    finalTranscriptRef.current = '';
    setVoiceState(prev => ({
      ...prev,
      transcript: '',
      finalTranscript: '',
      waitingForPermission: true
    }));
    setVoiceError(null);
    
    try {
      recognitionRef.current.start();
      console.log('ðŸŽ¤ æ¡Œé¢ç«¯å¼€å§‹è¯­éŸ³è¾“å…¥');
    } catch (error) {
      console.error('å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
      if (error.name === 'NotAllowedError') {
        setVoiceError('éº¦å…‹é£Žè®¿é—®è¢«æ‹’ç»ã€‚è¯·åˆ·æ–°é¡µé¢å¹¶å…è®¸éº¦å…‹é£Žæƒé™ã€‚');
        setVoiceState(prev => ({ ...prev, permissionState: 'denied' }));
      } else {
        setVoiceError(`å¯åŠ¨å¤±è´¥: ${error.message}`);
      }
    }
  }, [voiceState.isListening]);

  const stopVoiceInput = useCallback(() => {
    if (!recognitionRef.current || !voiceState.isListening) {
      return;
    }
    
    try {
      recognitionRef.current.stop();
      console.log('ðŸ›‘ æ¡Œé¢ç«¯åœæ­¢è¯­éŸ³è¾“å…¥');
    } catch (error) {
      console.error('åœæ­¢è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
    }
  }, [voiceState.isListening]);

  const clearVoiceError = useCallback(() => {
    setVoiceError(null);
  }, []);

  const speakText = useCallback((text) => {
    if (!voiceEnabled || !('speechSynthesis' in window)) return;
    
    try {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'zh-CN';
      utterance.rate = 1.0;
      utterance.pitch = 1.0;
      
      speechSynthesis.speak(utterance);
    } catch (error) {
      console.error('è¯­éŸ³æ’­æŠ¥å¤±è´¥:', error);
    }
  }, [voiceEnabled]);

  return {
    voiceState,
    voiceError,
    startVoiceInput,
    stopVoiceInput,
    clearVoiceError,
    speakText
  };
};